# -*- coding: utf-8 -*-
"""(Complete) Data Science - Machine Learning Project - Group Omicron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N7W3429lRHVxSzlOq-eUR1xj7h8NIJRo

# **Import Libraries**
"""

# import library
import pandas as pd
import numpy as np
import time

# import visualization library
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go

"""# **Read Dataset**"""

df = pd.read_csv('https://raw.githubusercontent.com/AnugrahLink26/Data-Science-2022/main/CSV%20File/WA_Fn-UseC_-Telco-Customer-Churn.csv', delimiter=',')
df.head()

"""# **Understanding Data**

The data set includes information about:
* **Customers who left within the last month** – the column is called Churn
* **Demographic** info about customers – gender, age range, and if they have partners and dependents
* **Services** that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies
* **Customer account information** - how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges
"""

df.shape

df.info()

df.columns.values

df.dtypes

"""# **Data Cleaning**

## Check Duplicate Data
"""

df.duplicated().sum()

"""There are no duplicate data

## Drop **customerID** column
"""

df = df.drop(['customerID'], axis = 1)
df.head()

"""## Check Missing Value

We see that **TotalCharges** data type is object, where it should be an float or integer, so we change it to a numeric and check the missing value
"""

# Converting Total Charges to a numerical data type.
df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')
df.isnull().sum()

"""We found 11 missing values in TotalCharges. Let's check this data!"""

df[np.isnan(df['TotalCharges'])]

"""We also found that the **tenure** column is 0 for these entries even though the MonthlyCharges column is not empty. Let's check if there are any other 0 values in the tenure column!

"""

df[df['tenure'] == 0].index

"""Now we found 11 data that has value 0 in **tenure**.

## Handling Missing Value

To handle missing values in TotalCharges column, we fill it with the **mean of TotalCharges values**.
"""

df.fillna(df["TotalCharges"].mean())

"""To handle **tenure** missing value, we decide to delete the rows with missing values in **tenure** columns since there are only 11 rows and deleting them will not affect the data."""

df.drop(labels=df[df['tenure'] == 0].index, axis=0, inplace=True)
df[df['tenure'] == 0].index

"""Let's check again the missing value!"""

df.isnull().sum()

"""Now there are no missing data

## Handling Outlier
To handle ourlier, We check the numerical columns (tenure, MonthlyCharges, TotalCharges) and plot it using Box Plot to find is there any outlier data and plot distribution data in Data Visualization so we can use Standardization for these numerical columns

# **Data Visualization**
"""

# Function to plot composition
def plot_composition(label, value, name, **kwargs):
  # Create subplots: use 'domain' type for Pie subplot
  fig = make_subplots(rows=1, cols=1, specs=[[{'type':'domain'}]])
  fig.add_trace(go.Pie(labels=label, values=value, name=name),
                1, 1)

  # Use `hole` to create a donut-like pie chart
  fig.update_traces(hole=.6, hoverinfo='label+percent+name', textfont_size=16)

  fig.update_layout(
      title_text='{} Composition'.format(name),
      # Add annotations in the center of the donut pies.
      annotations=[dict(text=name, x=0.5, y=0.5, font_size=20, showarrow=False)])
 
  fig.show()

# Function to boxplot
 def boxplot(y, text, title, **kwargs): 
  fig = px.box(df, x='Churn', y = y)

  # Update y-axis properties
  fig.update_yaxes(title_text=text, row=1, col=1)
  # Update x-axis properties
  fig.update_xaxes(title_text='Churn', row=1, col=1)

  # Update size and title
  fig.update_layout(autosize=True, width=750, height=600,
      title_font=dict(size=25, family='Courier'),
      title='<b>{} vs Churn</b>'.format(title),)

  fig.show()

# Function to plot correletation with Churn
def plot_correlation(hue, name, color, **kwargs):  
  sns.set(style="whitegrid")
  plt.figure(figsize=(10,8))
  total = float(len(df['gender']))
  ax = sns.countplot(x='Churn', hue=hue, data=df, palette=color)
  plt.title('Churn Composition Based on {}'.format(name), fontsize=20)
  for p in ax.patches:
      percentage = '{:.1f}%'.format(100 * p.get_height()/total)
      x = p.get_x() + p.get_width()
      y = p.get_height()
      ax.annotate(percentage, (x, y), ha='right', va='center',size=15, 
                  xytext=(0, 8), textcoords='offset points')
  plt.show()

numerical_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']
df[numerical_columns].describe()

"""## Churn"""

churn_labels = ['No', 'Yes']

plot_composition(churn_labels, df['Churn'].value_counts(), 'Churn')

"""There are 26.6% churn (Customers who left within the last month)

## Tenure vs Churn
"""

boxplot('tenure', 'Tenure (Months)', 'Tenure')

"""* New customers are more likely to churn
* There are outlier in tenure that the churn is yes, so we use Standardization for tenure column

### Distribution
"""

sns.set_context("notebook",font_scale=1.1)
plt.figure(figsize=(10,8))
ax = sns.kdeplot(df.tenure[(df["Churn"] == 'No') ],
                color="Red", shade = True);
ax = sns.kdeplot(df.tenure[(df["Churn"] == 'Yes') ],
                ax =ax, color="Blue", shade= True);
ax.legend(["Not Churn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Tenure');
ax.set_title('Distribution of tenure by churn');

df.groupby('Churn')['tenure'].describe()

"""## Monthly Charges vs Churn"""

boxplot('MonthlyCharges', 'Monthly Charges', 'Monthly Charges')

"""There are no outlier in MonthlyCharges column

### Distribution
"""

sns.set_context("notebook",font_scale=1.1)
plt.figure(figsize=(10,8))
ax = sns.kdeplot(df.MonthlyCharges[(df["Churn"] == 'No') ],
                color="Red", shade = True);
ax = sns.kdeplot(df.MonthlyCharges[(df["Churn"] == 'Yes') ],
                ax =ax, color="Blue", shade= True);
ax.legend(["Not Churn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Monthly Charges');
ax.set_title('Distribution of monthly charges by churn');

"""* Customers with higher Monthly Charges are also more likely to churn
* Because we now there is distribution for MonthlyCharges vs Churn, so we use Standardization for MonthlyCharges column
"""

df.groupby('Churn')['MonthlyCharges'].describe()

"""## Total Charges vs Churn"""

boxplot('TotalCharges', 'Total Charges', 'Total Charges')

"""There are outlier in TotalCharges that the churn is yes, so we use Standardization for TotalCharges column

### Distribution
"""

sns.set_context("notebook",font_scale=1.1)
plt.figure(figsize=(10,8))
ax = sns.kdeplot(df.TotalCharges[(df["Churn"] == 'No') ],
                color="Red", shade = True);
ax = sns.kdeplot(df.TotalCharges[(df["Churn"] == 'Yes') ],
                ax =ax, color="Blue", shade= True);
ax.legend(["Not Churn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Total Charges');
ax.set_title('Distribution of total charges by churn');

df.groupby('Churn')['TotalCharges'].describe()

"""## Gender"""

gender_labels = ['Male', 'Female']

plot_composition(gender_labels, df['gender'].value_counts(), 'Gender')

"""### Gender vs Churn"""

plot_correlation('gender', 'Gender', 'muted')

"""There is so little difference in customer percentage/ count who changed the service provider. Both genders behaved similarly when it comes to migrating to another service provider/firm.

## Payment Methods
"""

paymentmethod_labels = ['Electronic check', 'Mailed check', 
                        'Bank transfer (automatic)', 'Credit card (automatic)']

plot_composition(paymentmethod_labels, df['PaymentMethod'].value_counts(), 'Payment Methods')

"""### Payment Methods vs Churn"""

plot_correlation('PaymentMethod', 'Payment Method', 'muted')

"""* Major customer churn was having Electronic Check as Payment Method.
* Customers who choose for Credit-Card transfer (automatic) or Bank Transfer (automatic) and Mailed Checks as Payment Method were less likely to churn.

## Contract
"""

contract_labels = ['Month-to-month', 'One year', 'Two year']

plot_composition(contract_labels, df['Contract'].value_counts(), 'Contract')

"""### Contract vs Churn"""

plot_correlation('Contract', 'Contract', 'tab10')

"""About 23.5% of the customer with Month-to-Month Contracts choose to churn as compared to 2.4% of customers with One Year Contracts and 0.7% with Two Year Contract

## Phone Service
"""

phone_labels = ['No', 'Yes']

plot_composition(phone_labels, df['PhoneService'].value_counts(), 'Phone Service')

"""### Phone Service vs Churn"""

plot_correlation('PhoneService', 'Phone Service', 'muted')

"""Very few customers don't have phone service and out of that, about 26.6% of Customers are more likely to churn.

## Internet Service
"""

internet_labels = ['DSL', 'Fiber optic']

plot_composition(churn_labels, df['InternetService'].value_counts(), 'Internet Service')

"""### Internet Service vs Churn"""

plot_correlation('InternetService', 'Internet Service', 'muted')

"""## Multiple Lines"""

multiple_labels = ['No', 'Yes', 'No phone sevice']

plot_composition(multiple_labels, df['MultipleLines'].value_counts(), 'Multiple Lines')

"""### Multiple Lines vs Churn"""

plot_correlation('MultipleLines', 'Multiple Lines', 'muted')

"""* A lot of customers choose the Fiber optic service and it's also evident that the customers who use Fiber optic have a high churn rate, this might suggest dissatisfaction with this type of internet service.
* Customers having DSL service are the majority in number and have less churn rate compared to Fibre optic service.

## Online Security
"""

online_labels = ['No', 'Yes', 'No internet sevice']

plot_composition(online_labels, df['OnlineSecurity'].value_counts(), 'Online Security')

"""### Online Security vs Churn"""

plot_correlation('OnlineSecurity', 'Online Security', 'hls')

"""## Tech Support"""

tech_labels = ['No', 'Yes', 'No internet sevice']

plot_composition(tech_labels, df['TechSupport'].value_counts(), 'Tech Support')

"""### Tech Support vs Churn"""

plot_correlation('TechSupport', 'Tech Support', 'Paired')

"""## All Dataset Correlation"""

plt.figure(figsize=(25, 10))

corr = df.apply(lambda x: pd.factorize(x)[0]).corr()

mask = np.triu(np.ones_like(corr, dtype=bool))

ax = sns.heatmap(corr, mask=mask, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, linewidths=.2, cmap='coolwarm', vmin=-1, vmax=1)

"""# **Data Preprocessing**

## Library for Data Preprocessing
"""

from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import plot_confusion_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn import metrics
from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report
from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel

import warnings
warnings.filterwarnings('ignore')

"""## Feature Encoding

We use Label Encoding
"""

def object_to_int(dataframe_series):
  if dataframe_series.dtype=='object':
    dataframe_series = LabelEncoder().fit_transform(dataframe_series)
  return dataframe_series

df = df.apply(lambda x: object_to_int(x))
df.head()

"""## Correlation Coefficient with **Churn**"""

df.corr()['Churn'].sort_values(ascending = False)

"""## Train Test Split Data"""

X = df.drop(columns = ['Churn'])
y = df['Churn'].values

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state = 40, stratify=y)

"""## Standardizing Numeric Attributes

We use Standardization for numerical columns (tenure, MonthlyCharges, TotalCharges) because We know the Normal Distribution from Data Visualization and there are outlier in tenure and TotalCharges columns
"""

# Standardization tenure
X_train['tenure'] = stats.zscore(X_train['tenure'])
X_test['tenure'] = stats.zscore(X_test['tenure'])

# Standardization MonthlyCharges
X_train['MonthlyCharges'] = stats.zscore(X_train['MonthlyCharges'])
X_test['MonthlyCharges'] = stats.zscore(X_test['MonthlyCharges'])

# Standardization TotalCharges
X_train['TotalCharges'] = stats.zscore(X_train['TotalCharges'])
X_test['TotalCharges'] = stats.zscore(X_test['TotalCharges'])

X_train

"""# **Machine Learning Model Evaluations and Predictions**

## Import Packages
"""

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, roc_auc_score

"""## Logistic Regression"""

start = time.time()

lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

# membuat prediksi
lr_pred = lr_model.predict(X_test)
lr_report = classification_report(y_test,lr_pred)

# Calculate Model Performance
print('Logistic Regeression Accuracy  :','{:.3f}'.format(accuracy_score(y_test, lr_pred)))
print('Logistic Regeression Precision :','{:.3f}'.format(precision_score(y_test, lr_pred, average='macro')))  
print('Logistic Regeression Recall    :','{:.3f}'.format(recall_score(y_test, lr_pred, average='macro')))
print('Logistic Regeression F1-Score  :','{:.3f}\n'.format(f1_score(y_test, lr_pred, average='macro')))     
print(lr_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, lr_pred),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Logistic Regression Confusion Matrix",fontsize=14)
plt.show()
print()

# Logistic Regression ROC Curve
lr_pred_prob = lr_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, lr_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Logistic Regression',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();

print ("\nLogistic Regression ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, lr_pred_prob)))

end = time.time()
lr_time = end-start
print("\nThe time of execution of above program is :", lr_time)

"""## K Nearest Neighbour"""

start = time.time()

knn_model = KNeighborsClassifier(n_neighbors=10)
knn_model.fit(X_train, y_train)

# membuat prediksi
knn_pred = knn_model.predict(X_test)
knn_report = classification_report(y_test,knn_pred)

# Calculate Model Performance
print('KNN Accuracy  :','{:.3f}'.format(accuracy_score(y_test, knn_pred)))
print('KNN Precision :','{:.3f}'.format(precision_score(y_test, knn_pred, average='macro')))  
print('KNN Recall    :','{:.3f}'.format(recall_score(y_test, knn_pred, average='macro')))
print('KNN F1-Score  :','{:.3f}\n'.format(f1_score(y_test, knn_pred, average='macro')))   
print(knn_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, knn_pred),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("KNN Confusion Matrix",fontsize=14)
plt.show()
print()

# KNN ROC Curve
knn_pred_prob = knn_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, knn_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='KNN',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('KNN ROC Curve',fontsize=16)
plt.show();

print ("\nKNN ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, knn_pred_prob)))

end = time.time()
knn_time = end-start
print("\nThe time of execution of above program is :", knn_time)

"""## Decision Tree"""

start = time.time()

dt_model = DecisionTreeClassifier(criterion="entropy")
dt_model.fit(X_train, y_train)

# membuat prediksi
dt_pred = dt_model.predict(X_test)
dt_report = classification_report(y_test,dt_pred)

# Calculate Model Performance
print('Decision Tree Accuracy  :','{:.3f}'.format(accuracy_score(y_test, dt_pred))) 
print('Decision Tree Precision :','{:.3f}'.format(precision_score(y_test, dt_pred, average='macro')))  
print('Decision Tree Recall    :','{:.3f}'.format(recall_score(y_test, dt_pred, average='macro')))
print('Decision Tree F1-Score  :','{:.3f}\n'.format(f1_score(y_test, dt_pred, average='macro')))    
print(dt_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, dt_pred),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Decision Tree Confusion Matrix",fontsize=14)
plt.show()
print()

# Decision Tree ROC Curve
dt_pred_prob = dt_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, dt_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Decision Tree',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Decision Tree ROC Curve',fontsize=16)
plt.show();

print ("\nDecision Tree ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, dt_pred_prob)))

end = time.time()
dt_time = end-start
print("\nThe time of execution of above program is :", dt_time)

"""## Random Forest"""

start = time.time()

rf_model = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                random_state =50, max_features = "auto",
                                max_leaf_nodes = 30)
rf_model.fit(X_train, y_train)

# membuat prediksi
rf_pred = rf_model.predict(X_test)
rf_report = classification_report(y_test,rf_pred)

# Calculate Model Performance
print('Random Forest Accuracy  :','{:.3f}'.format(accuracy_score(y_test, rf_pred))) 
print('Random Forest Precision :','{:.3f}'.format(precision_score(y_test, rf_pred, average='macro')))  
print('Random Forest Recall    :','{:.3f}'.format(recall_score(y_test, rf_pred, average='macro')))
print('Random Forest F1-Score  :','{:.3f}\n'.format(f1_score(y_test, rf_pred, average='macro')))    
print(rf_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, rf_pred),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Random Forest Confusion Matrix",fontsize=14)
plt.show()
print()

# Random Forest ROC Curve
rf_pred_prob = rf_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, rf_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Random Forest',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest ROC Curve',fontsize=16)
plt.show();

print ("\nRandom Forest ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, rf_pred_prob)))

end = time.time()
rf_time = end-start
print("\nThe time of execution of above program is :", rf_time)

"""## AdaBoost"""

start = time.time()

ab_model = AdaBoostClassifier()
ab_model.fit(X_train, y_train)

# membuat prediksi
ab_pred = ab_model.predict(X_test)
ab_report = classification_report(y_test,ab_pred)

# Calculate Model Performance
print('AdaBoost Accuracy  :','{:.3f}'.format(accuracy_score(y_test, ab_pred)))
print('AdaBoost Precision :','{:.3f}'.format(precision_score(y_test, ab_pred, average='macro')))  
print('AdaBoost Recall    :','{:.3f}'.format(recall_score(y_test, ab_pred, average='macro')))
print('AdaBoost F1-Score  :','{:.3f}\n'.format(f1_score(y_test, ab_pred, average='macro')))     
print(ab_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, ab_pred),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("AdaBoost Confusion Matrix",fontsize=14)
plt.show()
print()

# AdaBoost ROC Curve
ab_pred_prob = ab_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, ab_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='AdaBoost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AdaBoost ROC Curve',fontsize=16)
plt.show();

print ("\nAdaBoost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, ab_pred_prob)))

end = time.time()
ab_time = end-start
print("\nThe time of execution of above program is :", ab_time)

"""## SVC"""

start = time.time()

svc_model = SVC(gamma='auto', probability=True)
svc_model.fit(X_train, y_train)

# membuat prediksi
svc_pred = svc_model.predict(X_test)
svc_report = classification_report(y_test,svc_pred)

# Calculate Model Performance
print('SVC Accuracy  :','{:.3f}'.format(accuracy_score(y_test, svc_pred))) 
print('SVC Precision :','{:.3f}'.format(precision_score(y_test, svc_pred, average='macro')))  
print('SVC Recall    :','{:.3f}'.format(recall_score(y_test, svc_pred, average='macro')))
print('SVC F1-Score  :','{:.3f}\n'.format(f1_score(y_test, svc_pred, average='macro')))    
print(svc_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, svc_pred),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("SVC Confusion Matrix",fontsize=14)
plt.show()
print()

# SVC ROC Curve
svc_pred_prob = svc_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, svc_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='SVC',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('SVC ROC Curve',fontsize=16)
plt.show();

print ("\nSVC ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, svc_pred_prob)))

end = time.time()
svc_time = end-start
print("\nThe time of execution of above program is :", svc_time)

"""## Gradient Boosting"""

start = time.time()

gb_model = GradientBoostingClassifier()
gb_model.fit(X_train, y_train)

# membuat prediksi
gb_pred = gb_model.predict(X_test)
gb_report = classification_report(y_test,gb_pred)

# Calculate Model Performance
print('Gradient Boosting Accuracy  :','{:.3f}'.format(accuracy_score(y_test, gb_pred))) 
print('Gradient Boosting Precision :','{:.3f}'.format(precision_score(y_test, gb_pred, average='macro')))  
print('Gradient Boosting Recall    :','{:.3f}'.format(recall_score(y_test, gb_pred, average='macro')))
print('Gradient Boosting F1-Score  :','{:.3f}\n'.format(f1_score(y_test, gb_pred, average='macro')))    
print(gb_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, gb_pred),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Gradient Boosting Confusion Matrix",fontsize=14)
plt.show()
print()

# Gradient Boosting ROC Curve
gb_pred_prob = gb_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, gb_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Gradient Boosting',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Gradient Boosting ROC Curve',fontsize=16)
plt.show();

print ("\nGradient Boosting ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, gb_pred_prob)))

end = time.time()
gb_time = end-start
print("\nThe time of execution of above program is :", gb_time)

"""## XG Boost"""

start = time.time()

xgb_model = XGBClassifier()
xgb_model.fit(X_train, y_train)

# membuat prediksi
xgb_pred = xgb_model.predict(X_test)
xgb_report = classification_report(y_test,xgb_pred)

# Calculate Model Performance
print('XG Boost Accuracy  :','{:.3f}'.format(accuracy_score(y_test, xgb_pred))) 
print('XG Boost Precision :','{:.3f}'.format(precision_score(y_test, xgb_pred, average='macro')))  
print('XG Boost Recall    :','{:.3f}'.format(recall_score(y_test, xgb_pred, average='macro')))
print('XG Boost F1-Score  :','{:.3f}\n'.format(f1_score(y_test, xgb_pred, average='macro')))    
print(xgb_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, xgb_pred),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("XG Boost Confusion Matrix",fontsize=14)
plt.show()
print()

# XG Boost ROC Curve
xgb_pred_prob = xgb_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, xgb_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='XG Boost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('XG Boost ROC Curve',fontsize=16)
plt.show();

print ("XG Boost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, xgb_pred_prob)))

end = time.time()
xgb_time = end-start
print("\nThe time of execution of above program is :", xgb_time)

"""## Execution Time"""

fig = plt.subplots(figsize =(20, 5))

# Create Data
ml_name = ['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest',
           'AdaBoost', 'SVC', 'Gradient Boosting', 'XG Boost']
ml_time = [lr_time, knn_time, dt_time, rf_time, 
           ab_time, svc_time, gb_time, xgb_time]
           
color = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'gray', 'pink']  
x = np.arange(len(ml_name)) 
width = 0.4    

# Plot Data
plt.bar(x, ml_time, width, color=color)
plt.title("Machine Learning Models Execution Time", fontweight = 'bold', fontsize = 20)
plt.xticks(x, ml_name, fontsize = 15)
plt.ylabel("Time (s)", fontsize = 15)
plt.show()

"""# **Handling Imbalanced Data Using ADASYN**"""

# Handling imbalance data
from imblearn.over_sampling import ADASYN
from collections import Counter

oversampling = ADASYN(random_state=0)

# Performing train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 40, stratify=y)

# Fit the over sampling
X_train_adsyn, y_train_adsyn = oversampling.fit_resample(X_train, y_train)

print("Before over sampling: {}".format(Counter(y_train)))
print("After over sampling: {}".format(Counter(y_train_adsyn)))

"""## Logistic Regression"""

start = time.time()

lr_model = LogisticRegression()
lr_model.fit(X_train_adsyn, y_train_adsyn)

# membuat prediksi
lr_pred_train = lr_model.predict(X_train_adsyn)
lr_pred_test = lr_model.predict(X_test)
lr_report = classification_report(y_test,lr_pred_test)

# Calculate Model Performance
print('Logistic Regeression Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, lr_pred_train)))
print('Logistic Regeression Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, lr_pred_test)))
print('Logistic Regeression Precision          :','{:.3f}'.format(precision_score(y_test, lr_pred_test, average='macro')))  
print('Logistic Regeression Recall             :','{:.3f}'.format(recall_score(y_test, lr_pred_test, average='macro')))
print('Logistic Regeression F1-Score           :','{:.3f}\n'.format(f1_score(y_test, lr_pred_test, average='macro')))     
print(lr_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, lr_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Logistic Regression Confusion Matrix",fontsize=14)
plt.show()
print()

# Logistic Regression ROC Curve
lr_pred_prob = lr_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, lr_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Logistic Regression',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();

print ("\nLogistic Regression ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, lr_pred_prob)))

end = time.time()
lr_time_adasyn = end-start
print("\nThe time of execution of above program is :", lr_time_adasyn)

"""## XG Boost"""

start = time.time()

xgb_model = XGBClassifier()
xgb_model.fit(X_train_adsyn, y_train_adsyn)

# membuat prediksi
xgb_pred_train = xgb_model.predict(X_train_adsyn)
xgb_pred_test = xgb_model.predict(X_test)
xgb_report = classification_report(y_test,xgb_pred_test)

# Calculate Model Performance
print('XG Boost Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, xgb_pred_train)))
print('XG Boost Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, xgb_pred_test)))
print('XG Boost Precision          :','{:.3f}'.format(precision_score(y_test, xgb_pred_test, average='macro')))  
print('XG Boost Recall             :','{:.3f}'.format(recall_score(y_test, xgb_pred_test, average='macro')))
print('XG Boost F1-Score           :','{:.3f}\n'.format(f1_score(y_test, xgb_pred_test, average='macro')))    
print(xgb_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, xgb_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("XG Boost Confusion Matrix",fontsize=14)
plt.show()
print()

# XG Boost ROC Curve
xgb_pred_prob = xgb_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, xgb_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='XG Boost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('XG Boost ROC Curve',fontsize=16)
plt.show();

print ("XG Boost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, xgb_pred_prob)))

end = time.time()
xgb_time_adasyn = end-start
print("\nThe time of execution of above program is :", xgb_time_adasyn)

"""## AdaBoost"""

start = time.time()

ab_model = AdaBoostClassifier()
ab_model.fit(X_train_adsyn, y_train_adsyn)

# membuat prediksi
ab_pred_train = ab_model.predict(X_train_adsyn)
ab_pred_test = ab_model.predict(X_test)
ab_report = classification_report(y_test,ab_pred_test)

# Calculate Model Performance
print('AdaBoost Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, ab_pred_train)))
print('AdaBoost Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, ab_pred_test)))
print('AdaBoost Precision          :','{:.3f}'.format(precision_score(y_test, ab_pred_test, average='macro')))  
print('AdaBoost Recall             :','{:.3f}'.format(recall_score(y_test, ab_pred_test, average='macro')))
print('AdaBoost F1-Score           :','{:.3f}\n'.format(f1_score(y_test, ab_pred_test, average='macro')))     
print(ab_report)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, ab_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("AdaBoost Confusion Matrix (Filter Method)",fontsize=14)
plt.show()
print()

# AdaBoost ROC Curve
ab_pred_prob = ab_model.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, ab_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='AdaBoost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AdaBoost ROC Curve',fontsize=16)
plt.show();

print ("\nAdaBoost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, ab_pred_prob)))

end = time.time()
ab_time_adasyn = end-start
print("\nThe time of execution of above program is :", ab_time_adasyn)

"""## Execution Time"""

fig = plt.subplots(figsize =(8, 8))

# Create Data
ml_best_name = ['Logistic Regression', 'XG Boost', 'AdaBoost']
ml_best_time = [lr_time, xgb_time, ab_time]

ml_adasyn_name = ['Before ADASYN', 'After ADASYN']
ml_time_adsyn = [lr_time_adasyn, xgb_time_adasyn, ab_time_adasyn]

x = np.arange(len(ml_best_name)) 
width = 0.2

# Plot Data
plt.bar(x-(width/2), ml_best_time, width, color='blue')
plt.bar(x+(width/2), ml_time_adsyn, width, color='red')
plt.title("Machine Learning Models Execution Time \n Before & After Using ADASYN", fontweight='bold', fontsize=20 )
plt.xticks(x, ml_best_name, fontsize=15)
plt.ylabel("Time (s)", fontsize=15)
plt.legend(ml_adasyn_name, fontsize=15)
plt.show()

"""# **Feature Selection**

## Filter Method
"""

filter = SelectKBest(f_classif, k=5)
filter.fit(X_train_adsyn, y_train_adsyn)

X_train_filter = filter.transform(X_train_adsyn)
X_test_filter = filter.transform(X_test)

print("Before feature selection", X_train_adsyn.shape)
print("After feature selection", X_train_filter.shape)
print("Score of features\n", filter.scores_)

feature_importance = pd.Series(filter.scores_, index=X_train_adsyn.columns)
feature_importance.sort_values().plot(kind='barh')
plt.show()

"""### Logistic Regression"""

start = time.time()

lr_model_filter = LogisticRegression()
lr_model_filter.fit(X_train_filter, y_train_adsyn)

# membuat prediksi
lr_pred_train = lr_model_filter.predict(X_train_filter)
lr_pred_test = lr_model_filter.predict(X_test_filter)
lr_report_filter = classification_report(y_test,lr_pred_test)

# Calculate Model Performance
print('Logistic Regeression Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, lr_pred_train)))
print('Logistic Regeression Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, lr_pred_test)))
print('Logistic Regeression Precision          :','{:.3f}'.format(precision_score(y_test, lr_pred_test, average='macro')))  
print('Logistic Regeression Recall             :','{:.3f}'.format(recall_score(y_test, lr_pred_test, average='macro')))
print('Logistic Regeression F1-Score           :','{:.3f}\n'.format(f1_score(y_test, lr_pred_test, average='macro')))     
print(lr_report_filter)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, lr_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Logistic Regression Confusion Matrix (Filter Method)",fontsize=14)
plt.show()
print()

# Logistic Regression ROC Curve
lr_pred_prob = lr_model_filter.predict_proba(X_test_filter)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, lr_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Logistic Regression',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();

print ("\nLogistic Regression ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, lr_pred_prob)))

end = time.time()
lr_time_filter = end-start
print("\nThe time of execution of above program is :", lr_time_filter)

"""### XG Boost"""

start = time.time()

xgb_model_filter = XGBClassifier()
xgb_model_filter.fit(X_train_filter, y_train_adsyn)

# membuat prediksi
xgb_pred_train = xgb_model_filter.predict(X_train_filter)
xgb_pred_test = xgb_model_filter.predict(X_test_filter)
xgb_report_filter = classification_report(y_test,xgb_pred_test)

# Calculate Model Performance
print('XG Boost Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, xgb_pred_train)))
print('XG Boost Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, xgb_pred_test)))
print('XG Boost Precision          :','{:.3f}'.format(precision_score(y_test, xgb_pred_test, average='macro')))  
print('XG Boost Recall             :','{:.3f}'.format(recall_score(y_test, xgb_pred_test, average='macro')))
print('XG Boost F1-Score           :','{:.3f}\n'.format(f1_score(y_test, xgb_pred_test, average='macro')))     
print(xgb_report_filter)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, xgb_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("XG Boost Confusion Matrix (Filter Method)",fontsize=14)
plt.show()
print()

# XG Boost ROC Curve
xgb_pred_prob = xgb_model_filter.predict_proba(X_test_filter)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, xgb_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='XG Boost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('XG Boost ROC Curve',fontsize=16)
plt.show();

print ("\nXG Boost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, xgb_pred_prob)))

end = time.time()
xgb_time_filter = end-start
print("\nThe time of execution of above program is :", xgb_time_filter)

"""### AdaBoost"""

start = time.time()

ab_model_filter = AdaBoostClassifier()
ab_model_filter.fit(X_train_filter, y_train_adsyn)

# membuat prediksi
ab_pred_train = ab_model_filter.predict(X_train_filter)
ab_pred_test = ab_model_filter.predict(X_test_filter)
ab_report_filter = classification_report(y_test,ab_pred_test)

# Calculate Model Performance
print('AdaBoost Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, ab_pred_train)))
print('AdaBoost Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, ab_pred_test)))
print('AdaBoost Precision          :','{:.3f}'.format(precision_score(y_test, ab_pred_test, average='macro')))  
print('AdaBoost Recall             :','{:.3f}'.format(recall_score(y_test, ab_pred_test, average='macro')))
print('AdaBoost F1-Score           :','{:.3f}\n'.format(f1_score(y_test, ab_pred_test, average='macro')))     
print(ab_report_filter)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, ab_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("AdaBoost Confusion Matrix (Filter Method)",fontsize=14)
plt.show()
print()

# AdaBoost ROC Curve
ab_pred_prob = ab_model_filter.predict_proba(X_test_filter)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, ab_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='AdaBoost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AdaBoost ROC Curve',fontsize=16)
plt.show();

print ("\nAdaBoost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, ab_pred_prob)))

end = time.time()
ab_time_filter = end-start
print("\nThe time of execution of above program is :", ab_time_filter)

"""## Wrapper Method (RFE)

### Logistic Regression
"""

lr_model_wrapper = LogisticRegression()

wrapper = RFE(lr_model_wrapper, n_features_to_select=5)
wrapper.fit(X_train_adsyn, y_train_adsyn)

X_train_wrapper = wrapper.transform(X_train_adsyn)
X_test_wrapper = wrapper.transform(X_test)

print("Before feature selection", X_train_adsyn.shape)
print("After feature selection", X_train_wrapper.shape)
print("Score of features", wrapper.ranking_)

feature_importance = pd.Series(wrapper.ranking_, index=X_train_adsyn.columns)
feature_importance.sort_values().plot(kind='barh')
plt.show()

start = time.time()

lr_model_wrapper=LogisticRegression()
lr_model_wrapper.fit(X_train_wrapper, y_train_adsyn)

# membuat prediksi
lr_pred_train = lr_model_wrapper.predict(X_train_wrapper)
lr_pred_test = lr_model_wrapper.predict(X_test_wrapper)
lr_report_wrapper = classification_report(y_test,lr_pred_test)

# Calculate Model Performance
print('Logistic Regeression Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, lr_pred_train)))
print('Logistic Regeression Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, lr_pred_test)))
print('Logistic Regeression Precision          :','{:.3f}'.format(precision_score(y_test, lr_pred_test, average='macro')))  
print('Logistic Regeression Recall             :','{:.3f}'.format(recall_score(y_test, lr_pred_test, average='macro')))
print('Logistic Regeression F1-Score           :','{:.3f}\n'.format(f1_score(y_test, lr_pred_test, average='macro')))     
print(lr_report_wrapper)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, lr_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Logistic Regression Confusion Matrix (Wrapper Method)",fontsize=14)
plt.show()
print()

# Logistic Regression ROC Curve
lr_pred_prob = lr_model_wrapper.predict_proba(X_test_wrapper)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, lr_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Logistic Regression',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();

print ("\nLogistic Regression ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, lr_pred_prob)))

end = time.time()
lr_time_wrapper = end-start
print("\nThe time of execution of above program is :", lr_time_wrapper)

"""### XG Boost"""

xgb_model_wrapper = XGBClassifier()

wrapper = RFE(xgb_model_wrapper, n_features_to_select=5)
wrapper.fit(X_train_adsyn, y_train_adsyn)

X_train_wrapper = wrapper.transform(X_train_adsyn)
X_test_wrapper = wrapper.transform(X_test)

print("Before feature selection", X_train_adsyn.shape)
print("After feature selection", X_train_wrapper.shape)
print("Score of features", wrapper.ranking_)

feature_importance = pd.Series(wrapper.ranking_, index=X_train_adsyn.columns)
feature_importance.sort_values().plot(kind='barh')
plt.show()

start = time.time()

xgb_model_wrapper=XGBClassifier()
xgb_model_wrapper.fit(X_train_wrapper, y_train_adsyn)

# membuat prediksi
xgb_pred_train = xgb_model_wrapper.predict(X_train_wrapper)
xgb_pred_test = xgb_model_wrapper.predict(X_test_wrapper)
xgb_report_wrapper = classification_report(y_test,xgb_pred_test)

# Calculate Model Performance
print('XG Boost Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, xgb_pred_train)))
print('XG Boost Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, xgb_pred_test)))
print('XG Boost Precision          :','{:.3f}'.format(precision_score(y_test, xgb_pred_test, average='macro')))  
print('XG Boost Recall             :','{:.3f}'.format(recall_score(y_test, xgb_pred_test, average='macro')))
print('XG Boost F1-Score           :','{:.3f}\n'.format(f1_score(y_test, xgb_pred_test, average='macro')))     
print(xgb_report_wrapper)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, xgb_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("XG Boost Confusion Matrix (Wrapper Method)",fontsize=14)
plt.show()
print()

# XG Boost ROC Curve
xgb_pred_prob = xgb_model_wrapper.predict_proba(X_test_wrapper)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, xgb_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='XG Boost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('XG Boost ROC Curve',fontsize=16)
plt.show();

print ("\nXG Boost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, xgb_pred_prob)))

end = time.time()
xgb_time_wrapper = end-start
print("\nThe time of execution of above program is :", xgb_time_wrapper)

"""### AdaBoost"""

ab_model_wrapper = AdaBoostClassifier()

wrapper = RFE(ab_model_wrapper, n_features_to_select=5)
wrapper.fit(X_train_adsyn, y_train_adsyn)

X_train_wrapper = wrapper.transform(X_train_adsyn)
X_test_wrapper = wrapper.transform(X_test)

print("Before feature selection", X_train_adsyn.shape)
print("After feature selection", X_train_wrapper.shape)
print("Score of features", wrapper.ranking_)

feature_importance = pd.Series(wrapper.ranking_, index=X_train_adsyn.columns)
feature_importance.sort_values().plot(kind='barh')
plt.show()

start = time.time()

ab_model_wrapper = AdaBoostClassifier()
ab_model_wrapper.fit(X_train_wrapper, y_train_adsyn)

# membuat prediksi
ab_pred_train = ab_model_wrapper.predict(X_train_wrapper)
ab_pred_test = ab_model_wrapper.predict(X_test_wrapper)
ab_report_wrapper = classification_report(y_test,ab_pred_test)

# Calculate Model Performance
print('AdaBoost Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, ab_pred_train)))
print('AdaBoost Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, ab_pred_test)))
print('AdaBoost Precision          :','{:.3f}'.format(precision_score(y_test, ab_pred_test, average='macro')))  
print('AdaBoost Recall             :','{:.3f}'.format(recall_score(y_test, ab_pred_test, average='macro')))
print('AdaBoost F1-Score           :','{:.3f}\n'.format(f1_score(y_test, ab_pred_test, average='macro')))     
print(ab_report_wrapper)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, ab_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("AdaBoost Confusion Matrix (Wrapper Method)",fontsize=14)
plt.show()
print()

# AdaBoost ROC Curve
ab_pred_prob = ab_model_wrapper.predict_proba(X_test_wrapper)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, ab_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='AdaBoost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AdaBoost ROC Curve',fontsize=16)
plt.show();

print ("\nAdaBoost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, ab_pred_prob)))

end = time.time()
ab_time_wrapper = end-start
print("\nThe time of execution of above program is :", ab_time_wrapper)

"""## Embedded Method

### Logistic Regression
"""

lr_model_importance = LogisticRegression()
lr_model_importance_feature = SelectFromModel(lr_model_importance)

lr_model_importance_feature.fit(X_train_adsyn, y_train_adsyn)

X_train_importance = lr_model_importance_feature.transform(X_train_adsyn)
X_test_importance = lr_model_importance_feature.transform(X_test)

print("Before feature selection", X_train_adsyn.shape)
print("After feature selection", X_train_importance.shape)
print("\nCoef\n",lr_model_importance_feature.estimator_.coef_[0])
print("Treshold",lr_model_importance_feature.threshold_)

feature_importance = pd.Series(lr_model_importance_feature.estimator_.coef_[0], index=X_train_adsyn.columns)
feature_importance.sort_values().plot(kind='barh')
plt.show()

start = time.time()

lr_model_importance=LogisticRegression()
lr_model_importance.fit(X_train_importance, y_train_adsyn)

# membuat prediksi
lr_pred_train = lr_model_importance.predict(X_train_importance)
lr_pred_test = lr_model_importance.predict(X_test_importance)
lr_report_importance = classification_report(y_test,lr_pred_test)

# Calculate Model Performance
print('Logistic Regeression Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, lr_pred_train)))
print('Logistic Regeression Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, lr_pred_test)))
print('Logistic Regeression Precision          :','{:.3f}'.format(precision_score(y_test, lr_pred_test, average='macro')))  
print('Logistic Regeression Recall             :','{:.3f}'.format(recall_score(y_test, lr_pred_test, average='macro')))
print('Logistic Regeression F1-Score           :','{:.3f}\n'.format(f1_score(y_test, lr_pred_test, average='macro')))     
print(lr_report_importance)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, lr_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Logistic Regression Confusion Matrix (Embedded Method)",fontsize=14)
plt.show()
print()

# Logistic Regression ROC Curve
lr_pred_prob = lr_model_importance.predict_proba(X_test_importance)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, lr_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='Logistic Regression',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();

print ("\nLogistic Regression ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, lr_pred_prob)))

end = time.time()
lr_time_importance = end-start
print("\nThe time of execution of above program is :", lr_time_importance)

"""### XG Boost"""

xgb_model_importance = XGBClassifier()
xgb_importance = SelectFromModel(xgb_model_importance)

xgb_importance.fit(X_train_adsyn, y_train_adsyn)

X_train_importance = xgb_importance.transform(X_train_adsyn)
X_test_importance = xgb_importance.transform(X_test)

print("Before feature selection", X_train_adsyn.shape)
print("After feature selection", X_train_importance.shape)
print("\nCoef\n",xgb_importance.estimator_.feature_importances_)
print("Treshold",xgb_importance.threshold_)

feature_importance = pd.Series(xgb_importance.estimator_.feature_importances_, index=X_train_adsyn.columns)
feature_importance.sort_values().plot(kind='barh')
plt.show()

start = time.time()

xgb_model_importance=XGBClassifier()
xgb_model_importance.fit(X_train_importance, y_train_adsyn)

# membuat prediksi
xgb_pred_train = xgb_model_importance.predict(X_train_importance)
xgb_pred_test = xgb_model_importance.predict(X_test_importance)
xgb_report_importance = classification_report(y_test,xgb_pred_test)

# Calculate Model Performance
print('XG Boost Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, xgb_pred_train)))
print('XG Boost Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, xgb_pred_test)))
print('XG Boost Precision          :','{:.3f}'.format(precision_score(y_test, xgb_pred_test, average='macro')))  
print('XG Boost Recall             :','{:.3f}'.format(recall_score(y_test, xgb_pred_test, average='macro')))
print('XG Boost F1-Score           :','{:.3f}\n'.format(f1_score(y_test, xgb_pred_test, average='macro')))     
print(xgb_report_importance)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, xgb_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("XG Boost Confusion Matrix (Embedded Method)",fontsize=14)
plt.show()
print()

# XG Boost ROC Curve
xgb_pred_prob = xgb_model_importance.predict_proba(X_test_importance)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, xgb_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='XG Boost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('XG Boost ROC Curve',fontsize=16)
plt.show();

print ("\nXG Boost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, xgb_pred_prob)))

end = time.time()
xgb_time_importance = end-start
print("\nThe time of execution of above program is :", xgb_time_importance)

"""### AdaBoost"""

ab_model_importance = AdaBoostClassifier()
ab_model_importance_feature = SelectFromModel(ab_model_importance)

ab_model_importance_feature.fit(X_train_adsyn, y_train_adsyn)

X_train_importance = ab_model_importance_feature.transform(X_train_adsyn)
X_test_importance = ab_model_importance_feature.transform(X_test)

print("Before feature selection", X_train_adsyn.shape)
print("After feature selection", X_train_importance.shape)
print("\nCoef\n",ab_model_importance_feature.estimator_.feature_importances_)
print("Treshold",ab_model_importance_feature.threshold_)

feature_importance = pd.Series(ab_model_importance_feature.estimator_.feature_importances_, index=X_train_adsyn.columns)
feature_importance.sort_values().plot(kind='barh')
plt.show()

start = time.time()

ab_model_importance = AdaBoostClassifier()
ab_model_importance.fit(X_train_importance, y_train_adsyn)

# membuat prediksi
ab_pred_train = ab_model_importance.predict(X_train_importance)
ab_pred_test = ab_model_importance.predict(X_test_importance)
ab_report_importance = classification_report(y_test,ab_pred_test)

# Calculate Model Performance
print('AdaBoost Training Accuracy  :','{:.3f}'.format(accuracy_score(y_train_adsyn, ab_pred_train)))
print('AdaBoost Test Accuracy      :','{:.3f}'.format(accuracy_score(y_test, ab_pred_test)))
print('AdaBoost Precision          :','{:.3f}'.format(precision_score(y_test, ab_pred_test, average='macro')))  
print('AdaBoost Recall             :','{:.3f}'.format(recall_score(y_test, ab_pred_test, average='macro')))
print('AdaBoost F1-Score           :','{:.3f}\n'.format(f1_score(y_test, ab_pred_test, average='macro')))     
print(ab_report_importance)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, ab_pred_test),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("AdaBoost Confusion Matrix (Embedded Method)",fontsize=14)
plt.show()
print()

# AdaBoost ROC Curve
ab_pred_prob = ab_model_importance.predict_proba(X_test_importance)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, ab_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr, tpr, label='AdaBoost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AdaBoost ROC Curve',fontsize=16)
plt.show();

print ("\nAdaBoost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, ab_pred_prob)))

end = time.time()
ab_time_importance = end-start
print("\nThe time of execution of above program is :", ab_time_importance)

"""## Execution Time"""

fig = plt.subplots(figsize =(10, 8))

# Create Data
ml_best_name = ['Logistic Regression', 'XG Boost', 'AdaBoost']
ml_feature = ['Filter Method', 'Wrapper Method', 'Embedded Method']

lr_time_fs = [lr_time_filter, lr_time_wrapper, lr_time_importance]
xgb_time_fs = [xgb_time_filter, xgb_time_wrapper, xgb_time_importance]
ab_time_fs = [ab_time_filter, ab_time_wrapper, ab_time_importance]

x = np.arange(len(ml_best_name)) 
width = 0.2

# Plot Data
plt.bar(x-width, lr_time_fs, width, color='cyan')
plt.bar(x, xgb_time_fs, width, color='orange')
plt.bar(x+width, ab_time_fs, width, color='magenta')
plt.title("Machine Learning Models Execution Time \n Feature Selection", fontweight ='bold', fontsize = 20)
plt.xticks(x, ml_feature, fontsize = 15)
plt.ylabel("Time (s)", fontsize = 15)
plt.legend(ml_best_name, fontsize = 15)
plt.show()

fig = plt.subplots(figsize =(8, 8))

# Create Data
ml_best_name = ['Logistic Regression', 'XG Boost', 'AdaBoost']
ml_best_time = [lr_time, xgb_time, ab_time]

ml_adasyn_filter = ['Before ADASYN', 'After ADASYN', 'Filter Method']
ml_time_adsyn = [lr_time_adasyn, xgb_time_adasyn, ab_time_adasyn]

ml_time_importance = [lr_time_importance, xgb_time_importance, ab_time_importance]

x = np.arange(len(ml_best_name))
width = 0.2

# Plot Data
plt.bar(x-width, ml_best_time, width, color='blue')
plt.bar(x, ml_time_adsyn, width, color='red')
plt.bar(x+width, ml_time_importance, width, color='green')
plt.title("Machine Learning Models Execution Time \n Comparison", fontweight='bold', fontsize=20 )
plt.xticks(x, ml_best_name, fontsize=15)
plt.ylabel("Time (s)", fontsize=15)
plt.legend(ml_adasyn_filter, fontsize=15)
plt.show()

"""# **Hyperparameter Tuning**

## Logistics Regression

### Cross  Validation
"""

cv_score = cross_val_score(lr_model, X, y, cv=10, scoring='recall')
print(cv_score)
print()
print("Average of accuracy using CV 10 fold ({:.5f}) and the standard deviation ({:.5f})".format(cv_score.mean(), cv_score.std()))

"""### Grid Search CV & Tuned Model"""

start = time.time()

grid_values = {'solver' : ['newton-cg', 'lbfgs', 'liblinear'],
              'penalty' : ['l1', 'l2'],
              'C' : [0.01, 0.1, 1, 10, 100]}

# Run a Grid Search CV over the hyperparameters
grid_objt_lr = GridSearchCV(lr_model, param_grid = grid_values, cv=10)

# Fit the model on the training data
grid_objt_lr.fit(X_train_importance, y_train_adsyn)

# show the best estimator that we have choosed
print(grid_objt_lr.best_estimator_)

# Predict Result
lr_pred_tuned = grid_objt_lr.best_estimator_.predict(X_test_importance)
lr_report_tuned = classification_report(y_test,lr_pred_tuned)

print('\nLogistic Regression Accuracy  :','{:.3f}'.format(accuracy_score(y_test, lr_pred_tuned))) 
print('Logistic Regression Precision :','{:.3f}'.format(precision_score(y_test, lr_pred_tuned, average='macro')))  
print('Logistic Regression Recall    :','{:.3f}'.format(recall_score(y_test, lr_pred_tuned, average='macro')))
print('Logistic Regression F1-Score  :','{:.3f}\n'.format(f1_score(y_test, lr_pred_tuned, average='macro')))    
print(lr_report_tuned)

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, lr_pred_tuned),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("Logistic Regression Confusion Matrix (Tuned)",fontsize=14)
plt.show()
print()

# Logistic Regression ROC Curve
lr_prob_tuned = grid_objt_lr.best_estimator_.predict_proba(X_test_importance)[:,1]

fpr_rf, tpr_rf, thresholds = roc_curve(y_test, lr_prob_tuned)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr_rf, tpr_rf, label='Logistic Regression',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve',fontsize=16)
plt.show();

print ("\nLogistic Regression ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, lr_prob_tuned)))

end = time.time()
lr_time_tuning = end-start
print("\nThe time of execution of above program is :", lr_time_tuning)

"""## XG Boost

### Cross Validation
"""

cv_score = cross_val_score(xgb_model, X, y, cv=10, scoring='recall')
print(cv_score)
print()
print("Average of accuracy using CV 10 fold ({:.5f}) and the standard deviation ({:.5f})".format(cv_score.mean(), cv_score.std()))

"""### Grid Search CV & Tuned Model"""

start = time.time()

grid_values = {'max_depth': [2, 3, 5, 10, 20],
              'booster': ["gbtree", "gblinear","dart"]}

# Run a Grid Search CV over the hyperparameters
grid_objt_xgb = GridSearchCV(xgb_model, param_grid = grid_values, cv=10)

# Fit the model on the training data
grid_objt_xgb.fit(X_train_importance, y_train_adsyn)

# show the best estimator that we have choosed
grid_objt_xgb.best_estimator_

# Predict Result
xgb_pred_tuned = grid_objt_xgb.best_estimator_.predict(X_test_importance)
xgb_report_tuned = classification_report(y_test,xgb_pred_tuned)

print('XG Boost Accuracy  :','{:.3f}'.format(accuracy_score(y_test, xgb_pred_tuned))) 
print('XG Boost Precision :','{:.3f}'.format(precision_score(y_test, xgb_pred_tuned, average='macro')))  
print('XG Boost Recall    :','{:.3f}'.format(recall_score(y_test, xgb_pred_tuned, average='macro')))
print('XG Boost F1-Score  :','{:.3f}\n'.format(f1_score(y_test, xgb_pred_tuned, average='macro')))  
print(xgb_report_tuned)  

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, xgb_pred_tuned),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("XG Boost Confusion Matrix (Tuned)",fontsize=14)
plt.show()
print()

# XG Boost ROC Curve
xgb_prob_tuned = grid_objt_xgb.best_estimator_.predict_proba(X_test_importance)[:,1]

fpr_rf, tpr_rf, thresholds = roc_curve(y_test, xgb_prob_tuned)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr_rf, tpr_rf, label='XG Boost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('XG Boost ROC Curve',fontsize=16)
plt.show();

print ("\nXG Boost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, xgb_prob_tuned)))

end = time.time()
xgb_time_tuning = end-start
print("\nThe time of execution of above program is :", xgb_time_tuning)

"""## AdaBoost

### Cross Validation
"""

cv_score = cross_val_score(ab_model, X, y, cv=10, scoring='recall')
print(cv_score)
print()
print("Average of accuracy using CV 10 fold ({:.5f}) and the standard deviation ({:.5f})".format(cv_score.mean(), cv_score.std()))

"""### Grid Search CV & Tuned Model"""

start = time.time()

grid_values = {"n_estimators": [100, 200],
              "learning_rate": [0.001, 0.01, 0.1, 0.2, 0.5]}

# Run a Grid Search CV over the hyperparameters
grid_objt_ab = GridSearchCV(ab_model, param_grid=grid_values, cv=10)

# Fit the model on the training data
grid_objt_ab.fit(X_train_importance, y_train_adsyn)

# show the best estimator that we have choosed
grid_objt_ab.best_estimator_

# Predict Result
ab_pred_tuned = grid_objt_ab.best_estimator_.predict(X_test_importance)
ab_report_tuned = classification_report(y_test,ab_pred_tuned)

print('AdaBoost Accuracy  :','{:.3f}'.format(accuracy_score(y_test, ab_pred_tuned))) 
print('AdaBoost Precision :','{:.3f}'.format(precision_score(y_test, ab_pred_tuned, average='macro')))  
print('AdaBoost Recall    :','{:.3f}'.format(recall_score(y_test, ab_pred_tuned, average='macro')))
print('AdaBoost F1-Score  :','{:.3f}\n'.format(f1_score(y_test, ab_pred_tuned, average='macro'))) 
print(ab_report_tuned)   

# Confusion Matrix
plt.figure(figsize=(4,3))
sns.heatmap(confusion_matrix(y_test, ab_pred_tuned),
            annot=True,fmt = "d",linecolor="k",linewidths=3)
    
plt.title("AdaBoost Confusion Matrix (Tuned)",fontsize=14)
plt.show()
print()

# AdaBoost ROC Curve
ab_pred_tuned = grid_objt_ab.best_estimator_.predict_proba(X_test_importance)[:,1]

fpr_ab, tpr_ab, thresholds = roc_curve(y_test, ab_pred_tuned)
plt.plot([0, 1], [0, 1], 'k--' )
plt.plot(fpr_ab, tpr_ab, label='AdaBoost',color = "r")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AdaBoost ROC Curve',fontsize=16)
plt.show();

print ("\nAdaBoost ROC Score :", '{:.3f}'.format(roc_auc_score(y_test, ab_pred_tuned)))

end = time.time()
ab_time_tuning = end-start
print("\nThe time of execution of above program is :", ab_time_tuning)

"""## Execution Time"""

fig = plt.subplots(figsize =(8, 8))

# Create Data
ml_best_name = ['Logistic Regression', 'XG Boost', 'AdaBoost']
ml_best_time = [lr_time, xgb_time, ab_time]

ml_adasyn_filter_tuning = ['Before ADASYN', 'After ADASYN', 'Filter Method', 'Grid Search CV']
ml_time_adsyn = [lr_time_adasyn, xgb_time_adasyn, ab_time_adasyn]

ml_time_filter = [lr_time_filter, xgb_time_filter, ab_time_filter]
ml_time_tuning = [lr_time_tuning, xgb_time_tuning, ab_time_tuning]

x = np.arange(len(ml_best_name))
width = 0.2

# Plot Data
plt.bar(x-(3*width/2), ml_best_time, width, color='blue')
plt.bar(x-(width/2), ml_time_adsyn, width, color='red')
plt.bar(x+(width/2), ml_time_filter, width, color='green')
plt.bar(x+(3*width/2), ml_time_tuning, width, color='gold')
plt.title("Machine Learning Models Execution Time \n Comparison", fontweight='bold', fontsize=20 )
plt.xticks(x, ml_best_name, fontsize=15)
plt.ylabel("Time (s)", fontsize=15)
plt.legend(ml_adasyn_filter_tuning, fontsize=15)
plt.show()

"""# **Explainable AI**"""

!pip install shap

import shap

"""## **With Handling Imbalanced dataset**"""

clf = LogisticRegression(C=0.1, solver='newton-cg')
clf.fit(X_train, y_train)

explainer = shap.LinearExplainer(clf, X_train)
shap_values = explainer.shap_values(X_test)
print('Expected Values',explainer.expected_value)

"""The above metric of **expected value is -1.59** will be used as **base value**. The value above the base value will predict the the customers who is churn (target = 1) and vice versa (target = 0)

The features in **red** one push the prediction to predict the customers who is churn and **blue** one push the prediction to predict otherwise.
"""

# Evaluation
y_predict_train = clf.predict(X_train)
y_predict_test = clf.predict(X_test)

training_acc = accuracy_score(y_train, y_predict_train)
testing_acc = accuracy_score(y_test, y_predict_test)

print("Training Accuracy: {}".format(training_acc))
print("Testing Accuracy: {}".format(testing_acc))

print(classification_report(y_test, y_predict_test))

y_test_pd = pd.DataFrame(y_test,columns=['Churn'])
y_test_pd.head()

y_pred_test_pd = pd.DataFrame(y_predict_test, columns=['Result'])
y_pred_test_pd.head()

X_test = X_test.reset_index(drop=True)
y_test = y_test_pd.reset_index(drop=True)

prediction = y_test
prediction['result'] = y_pred_test_pd
prediction['Summarize'] = prediction['result'] + prediction['Churn']
prediction[prediction['Summarize'] == 0].head()

prediction[prediction['Summarize'] == 2]

shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[2071,:],
                X_test.iloc[2107,:])